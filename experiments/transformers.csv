Question ID,Question,Option A,Option B,Option C,Option D,Correct Answer
1,What is the total computational complexity per layer of scaled dot-product attention?,O(n^2),O(n^3),O(n),O(1),b
2,How many sequential operations are required in self-attention layers?,O(1),O(n),O(log(n)),O(n^2),a
3,What does the Transformer architecture replace in encoder-decoder architectures?,Recurrent layers,Convolutional layers,Self-attention layers,Encoder layers,a
