{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    #repo_id = 'mistralai/Mistral-7B-v0.1',\n",
    "    repo_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64, \"max_new_tokens\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/ujjwaltiwari/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <|system|>\n",
      "You are an AI assistant that follows instruction extremely well.\n",
      "Please be truthful and give direct answers\n",
      "</s>\n",
      " <|user|>\n",
      " What is capital of India and UAE?\n",
      " </s>\n",
      " <|assistant|>\n",
      " The capital of India is New Delhi and the capital of the United Arab Emirates (UAE) is Abu Dhabi.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is capital of India and UAE?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    " <|system|>\n",
    "You are an AI assistant that follows instruction extremely well.\n",
    "Please be truthful and give direct answers\n",
    "</s>\n",
    " <|user|>\n",
    " {query}\n",
    " </s>\n",
    " <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice questions\",\n",
    "        \"options\" : {\n",
    "            \"a\" : \"choice here\",\n",
    "            \"b\" : \"choice here\",\n",
    "            \"c\" : \"choice here\",\n",
    "            \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "    \"2\" : {\n",
    "        \"mcq\" : \"multiple choice question\",\n",
    "        \"options\" : {\n",
    "            \"a\" : \"choice here\",\n",
    "            \"b\" : \"choice here\",\n",
    "            \"c\" : \"choice here\",\n",
    "            \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "    \"3\" : {\n",
    "        \"mcq\" : \"multiple choice question\",\n",
    "        \"options\" : {\n",
    "            \"a\" : \"choice here\",\n",
    "            \"b\" : \"choice here\",\n",
    "            \"c\" : \"choice here\",\n",
    "            \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "###Identifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=['text', 'number', 'subject', 'tone', 'response_json'],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key='quiz', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "###Identifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"review\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(chains=[quiz_chain, review_chain], input_variables=['text','number','subject','tone','response_json'],\n",
    "                                            output_variables=['quiz','review'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/ujjwaltiwari/Desktop/projects/MCQGen/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 3\n",
    "SUBJECT= 'Self Attention'\n",
    "TONE= 'simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:A paper on a new simple network architecture, the Transformer, based solely on attention mechanisms\n",
      "\n",
      "The NIPS 2017 accepted paper, Attention Is All You Need, introduces Transformer, a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. This paper is authored by professionals from the Google research team including Ashish Vaswani, Noam Shazeer, Niki Parmar,  Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
      "\n",
      "The Transformer – Attention is all you need\n",
      "What problem is the paper attempting to solve?\n",
      "Recurrent neural networks (RNN), long short-term memory networks(LSTM) and gated RNNs are the popularly approaches used for Sequence Modelling tasks such as machine translation and language modeling. However, RNN/CNN handle sequences word-by-word in a sequential fashion. This sequentiality is an obstacle toward parallelization of the process. Moreover, when such sequences are too long, the model is prone to forgetting the content of distant positions in sequence or mix it with following positions’ content.\n",
      "\n",
      "Recent works have achieved significant improvements in computational efficiency and model performance through factorization tricks and conditional computation. But they are not enough to eliminate the fundamental constraint of sequential computation.\n",
      "\n",
      "Attention mechanisms are one of the solutions to overcome the problem of model forgetting. This is because they allow dependency modelling without considering their distance in the input or output sequences. Due to this feature, they have become an integral part of sequence modeling and transduction models. However, in most cases attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "Paper summary\n",
      "The Transformer proposed in this paper is a model architecture which relies entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and tremendously improves translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "Neural sequence transduction models generally have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output sequence of symbols, one element at a time.\n",
      "\n",
      "architecture of the transformer\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "\n",
      "The authors are motivated to use self-attention because of three criteria.  \n",
      "\n",
      "One is that the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "The Transformer uses two different types of attention functions:\n",
      "\n",
      "Scaled Dot-Product Attention, computes the attention function on a set of queries simultaneously, packed together into a matrix.\n",
      "\n",
      "Multi-head attention, allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality, which is often the case with machine translations\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of 3 multiple choice questions for Self Attention students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 3 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice questions\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "###Identifier\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:A paper on a new simple network architecture, the Transformer, based solely on attention mechanisms\n",
      "\n",
      "The NIPS 2017 accepted paper, Attention Is All You Need, introduces Transformer, a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. This paper is authored by professionals from the Google research team including Ashish Vaswani, Noam Shazeer, Niki Parmar,  Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
      "\n",
      "The Transformer – Attention is all you need\n",
      "What problem is the paper attempting to solve?\n",
      "Recurrent neural networks (RNN), long short-term memory networks(LSTM) and gated RNNs are the popularly approaches used for Sequence Modelling tasks such as machine translation and language modeling. However, RNN/CNN handle sequences word-by-word in a sequential fashion. This sequentiality is an obstacle toward parallelization of the process. Moreover, when such sequences are too long, the model is prone to forgetting the content of distant positions in sequence or mix it with following positions’ content.\n",
      "\n",
      "Recent works have achieved significant improvements in computational efficiency and model performance through factorization tricks and conditional computation. But they are not enough to eliminate the fundamental constraint of sequential computation.\n",
      "\n",
      "Attention mechanisms are one of the solutions to overcome the problem of model forgetting. This is because they allow dependency modelling without considering their distance in the input or output sequences. Due to this feature, they have become an integral part of sequence modeling and transduction models. However, in most cases attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "Paper summary\n",
      "The Transformer proposed in this paper is a model architecture which relies entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and tremendously improves translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "Neural sequence transduction models generally have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output sequence of symbols, one element at a time.\n",
      "\n",
      "architecture of the transformer\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "\n",
      "The authors are motivated to use self-attention because of three criteria.  \n",
      "\n",
      "One is that the total computational complexity per layer.\n",
      "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "The Transformer uses two different types of attention functions:\n",
      "\n",
      "Scaled Dot-Product Attention, computes the attention function on a set of queries simultaneously, packed together into a matrix.\n",
      "\n",
      "Multi-head attention, allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n",
      "\n",
      "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality, which is often the case with machine translations\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of 3 multiple choice questions for Self Attention students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 3 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice questions\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "###Identifier\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response=generate_evaluate_chain(\n",
    "    {\n",
    "        \"text\": TEXT,\n",
    "        \"number\": NUMBER,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"tone\" : TONE,\n",
    "        \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nText:A paper on a new simple network architecture, the Transformer, based solely on attention mechanisms\\n\\nThe NIPS 2017 accepted paper, Attention Is All You Need, introduces Transformer, a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. This paper is authored by professionals from the Google research team including Ashish Vaswani, Noam Shazeer, Niki Parmar,  Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\\n\\nThe Transformer – Attention is all you need\\nWhat problem is the paper attempting to solve?\\nRecurrent neural networks (RNN), long short-term memory networks(LSTM) and gated RNNs are the popularly approaches used for Sequence Modelling tasks such as machine translation and language modeling. However, RNN/CNN handle sequences word-by-word in a sequential fashion. This sequentiality is an obstacle toward parallelization of the process. Moreover, when such sequences are too long, the model is prone to forgetting the content of distant positions in sequence or mix it with following positions’ content.\\n\\nRecent works have achieved significant improvements in computational efficiency and model performance through factorization tricks and conditional computation. But they are not enough to eliminate the fundamental constraint of sequential computation.\\n\\nAttention mechanisms are one of the solutions to overcome the problem of model forgetting. This is because they allow dependency modelling without considering their distance in the input or output sequences. Due to this feature, they have become an integral part of sequence modeling and transduction models. However, in most cases attention mechanisms are used in conjunction with a recurrent network.\\n\\nPaper summary\\nThe Transformer proposed in this paper is a model architecture which relies entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and tremendously improves translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\nNeural sequence transduction models generally have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output sequence of symbols, one element at a time.\\n\\narchitecture of the transformer\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\\n\\nThe authors are motivated to use self-attention because of three criteria.  \\n\\nOne is that the total computational complexity per layer.\\nAnother is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network.\\nThe Transformer uses two different types of attention functions:\\n\\nScaled Dot-Product Attention, computes the attention function on a set of queries simultaneously, packed together into a matrix.\\n\\nMulti-head attention, allows the model to jointly attend to information from different representation subspaces at different positions.\\n\\nA self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\\n\\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality, which is often the case with machine translations\\nYou are an expert MCQ maker. Given the above text, it is your job to create a quiz of 3 multiple choice questions for Self Attention students in simple tone.\\nMake sure the questions are not repeated and check all the questions to be conforming the text as well.\\nMake sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 3 MCQs\\n### RESPONSE_JSON\\n{\"1\": {\"mcq\": \"multiple choice questions\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\\n###Identifier\\n1\\n###Question\\nWhat is the motivation behind using self-attention in the Transformer model?\\n###Options\\na) Self-attention allows the model to jointly attend to information from different representation subspaces at different positions\\nb) Self-attention connects all positions with a constant number of sequentially executed operations\\nc) Self-attention is faster than recurrent layers when the sequence length is smaller than the representation dimensionality\\nd) All of the above\\n###Correct\\nd) All of the above\\n###Identifier\\n2\\n###Question\\nWhat is the maximum number of sequential operations required for self-attention layers?\\n###Options\\na) O(n)\\nb) O(log n)\\nc) O(1)\\nd) O(n log n)\\n###Correct\\nc) O(1)\\n###Identifier\\n3\\n###Question\\nWhat is the computational complexity of self-attention layers compared to recurrent layers?\\n###Options\\na) Self-attention layers are slower than recurrent layers\\nb) Self-attention layers have the same computational complexity as recurrent layers\\nc) Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality\\nd) Self-attention layers are always faster than recurrent layers\\n###Correct\\nc) Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=response.get('quiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?<=###Identifier).*\"\n",
    "match = re.search(pattern, text, re.DOTALL)\n",
    "result = match.group(0).strip()  # Remove leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n###Question\\nWhat is the motivation behind using self-attention in the Transformer model?\\n###Options\\na) Self-attention allows the model to jointly attend to information from different representation subspaces at different positions\\nb) Self-attention connects all positions with a constant number of sequentially executed operations\\nc) Self-attention is faster than recurrent layers when the sequence length is smaller than the representation dimensionality\\nd) All of the above\\n###Correct\\nd) All of the above\\n###Identifier\\n2\\n###Question\\nWhat is the maximum number of sequential operations required for self-attention layers?\\n###Options\\na) O(n)\\nb) O(log n)\\nc) O(1)\\nd) O(n log n)\\n###Correct\\nc) O(1)\\n###Identifier\\n3\\n###Question\\nWhat is the computational complexity of self-attention layers compared to recurrent layers?\\n###Options\\na) Self-attention layers are slower than recurrent layers\\nb) Self-attention layers have the same computational complexity as recurrent layers\\nc) Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality\\nd) Self-attention layers are always faster than recurrent layers\\n###Correct\\nc) Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data=json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Option A</th>\n",
       "      <th>Option B</th>\n",
       "      <th>Option C</th>\n",
       "      <th>Option D</th>\n",
       "      <th>Correct Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the total computational complexity per...</td>\n",
       "      <td>O(n^2)</td>\n",
       "      <td>O(n^3)</td>\n",
       "      <td>O(n)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>How many sequential operations are required in...</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n)</td>\n",
       "      <td>O(log(n))</td>\n",
       "      <td>O(n^2)</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What does the Transformer architecture replace...</td>\n",
       "      <td>Recurrent layers</td>\n",
       "      <td>Convolutional layers</td>\n",
       "      <td>Self-attention layers</td>\n",
       "      <td>Encoder layers</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question ID                                           Question  \\\n",
       "0           1  What is the total computational complexity per...   \n",
       "1           2  How many sequential operations are required in...   \n",
       "2           3  What does the Transformer architecture replace...   \n",
       "\n",
       "           Option A              Option B               Option C  \\\n",
       "0            O(n^2)                O(n^3)                   O(n)   \n",
       "1              O(1)                  O(n)              O(log(n))   \n",
       "2  Recurrent layers  Convolutional layers  Self-attention layers   \n",
       "\n",
       "         Option D Correct Answer  \n",
       "0            O(1)              b  \n",
       "1          O(n^2)              a  \n",
       "2  Encoder layers              a  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for key, value in json_data.items():\n",
    "            mcq = value.get('mcq', '')\n",
    "            options = value.get('options', {})\n",
    "            correct = value.get('correct', '')\n",
    "            row = {\n",
    "                'Question ID': key,\n",
    "                'Question': mcq,\n",
    "                'Option A': options.get('a', ''),\n",
    "                'Option B': options.get('b', ''),\n",
    "                'Option C': options.get('c', ''),\n",
    "                'Option D': options.get('d', ''),\n",
    "                'Correct Answer': correct\n",
    "            }\n",
    "            rows.append(row)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"transformers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
